{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 自然言語処理のキソ\n",
    "NLP;Natural Language Processing\n",
    "\n",
    "- 本発表では文書同士の類似度の比較方法を例に見ていきます\n",
    "\n",
    "法政大学　卒業生　湯浅大地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 自然言語\n",
    "- 自然言語とは、私たちが普段使っている言葉のこと\n",
    "- プログラミング言語などの人工的に作られた言語の対比として「自然」と呼ばれる\n",
    "\n",
    "## 自然言語処理のやりたいこと\n",
    "- 自然言語を数学的に表現する⇨機械学習、AIに応用することができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  自然言語処理の世界に一緒に踏み入れましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ケーススタディ\n",
    "一番似ている２人を探せ！！\n",
    "\n",
    "- Aさん：ステーキが好きです。ハンバーグも好きです。\n",
    "- Bさん：ステーキがとても好きです。\n",
    "- Cさん：パスタが好きです。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## どうやって探す？\n",
    "\n",
    "1.文書を数学的に表現にする\n",
    "- Bag-Of-Words（Bow）の手法を用いてベクトルで表現する\n",
    "\n",
    "2.文書の比較\n",
    "- コサイン類似度の手法を用いる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.文書を数学的に表現にする\n",
    "## まずは単語分解！！\n",
    "専門的には形態素解析といいます。\n",
    "\n",
    "- 文章を名詞、動詞、形容詞などの品詞別に分解する\n",
    "- 日本語は英語と違って文章が単語ごとに区切られていない\n",
    "- 日本語を形態素解析できるソフトウェアとして広く使われているのはMeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MeCab\n",
    "\n",
    "- 京都大学とNTTの基礎研究所が共同開発したもの\n",
    "- オープンソースソフトウェアでダウンロード、使用が全て無料\n",
    "- 内蔵されているIPA辞書という日本語辞書には約40万語の単語が登録されている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 『ステーキとハンバーグが好きです。 』を単語分解！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS/EOS,*,*,*,*,*,*,*,*\n",
      "名詞,一般,*,*,*,*,ステーキ,ステーキ,ステーキ\n",
      "助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "名詞,形容動詞語幹,*,*,*,*,好き,スキ,スキ\n",
      "助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "記号,句点,*,*,*,*,。,。,。\n",
      "名詞,一般,*,*,*,*,ハンバーグ,ハンバーグ,ハンバーグ\n",
      "助詞,係助詞,*,*,*,*,も,モ,モ\n",
      "名詞,形容動詞語幹,*,*,*,*,好き,スキ,スキ\n",
      "助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "記号,句点,*,*,*,*,。,。,。\n",
      "BOS/EOS,*,*,*,*,*,*,*,*\n"
     ]
    }
   ],
   "source": [
    "# MeCabライブラリをインポート\n",
    "import MeCab\n",
    "# パーサーの設定。ChaSenという形態素解析器と互換の出力をする設定にしている。\n",
    "tagger = MeCab.Tagger('-Ochasen')\n",
    "# 形態素解析\n",
    "testNode = tagger.parseToNode(\"ステーキが好きです。ハンバーグも好きです。\")\n",
    "while testNode:\n",
    "    print(testNode.feature)\n",
    "    testNode = testNode.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# 形態素解析の関数\n",
    "def word_tokenaize(doc):\n",
    "    node = tagger.parseToNode(doc)\n",
    "\n",
    "    result = []\n",
    "    while node:\n",
    "        hinshi = node.feature.split(\",\")[0]\n",
    "        if hinshi == '名詞' or hinshi == '動詞':\n",
    "            result.append(node.feature.split(\",\")[6])\n",
    "        node = node.next\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 文章を品詞分解したが、品詞には、一人一人の特徴とは関係ないものも含まれている\n",
    "\n",
    "- 「と」「が」「です」「。」\n",
    "\n",
    "## このような特徴とは関係のない単語のことをストップワード（Stop Words）と呼ばれる\n",
    "\n",
    "- 単語を選別することで、意義深い結果が得やすくなるだけでなく、計算負荷を減らすことができる\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ステーキ 好き ハンバーグ 好き'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ストップワードを除去した後の単語分解の結果\n",
    "word_tokenaize(\"ステーキが好きです。ハンバーグも好きです。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': [\"ステーキが好きです。ハンバーグも好きです。\", \"ステーキがとても好きです。\", \"パスタが好きです。\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ステーキが好きです。ハンバーグも好きです。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ステーキがとても好きです。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>パスタが好きです。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       x\n",
       "0  ステーキが好きです。ハンバーグも好きです。\n",
       "1          ステーキがとても好きです。\n",
       "2              パスタが好きです。"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df['wakati'] = df['x'].apply(word_tokenaize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3人の文章をストップワード除いて単語分解してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>wakati</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ステーキが好きです。ハンバーグも好きです。</td>\n",
       "      <td>ステーキ 好き ハンバーグ 好き</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ステーキがとても好きです。</td>\n",
       "      <td>ステーキ 好き</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>パスタが好きです。</td>\n",
       "      <td>パスタ 好き</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       x            wakati\n",
       "0  ステーキが好きです。ハンバーグも好きです。  ステーキ 好き ハンバーグ 好き\n",
       "1          ステーキがとても好きです。           ステーキ 好き\n",
       "2              パスタが好きです。            パスタ 好き"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# やっと！文章を数学的に表現してみる\n",
    "\n",
    "## Bag-of-Words\n",
    "- 単語にベクトルに各列を割り当てておいて、出現回数などを要素とすることで文章をベクトル化したもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "bag = vectorizer.fit_transform(df['wakati'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_bag = pd.DataFrame(data=bag.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ステーキ</th>\n",
       "      <th>ハンバーグ</th>\n",
       "      <th>パスタ</th>\n",
       "      <th>好き</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ステーキ  ハンバーグ  パスタ  好き\n",
       "0     1      1    0   2\n",
       "1     1      0    0   1\n",
       "2     0      0    1   1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ついに！文章同士の類似度を算出\n",
    "\n",
    "## コサイン類似度\n",
    "- 算出方法\n",
    "\n",
    "$$\n",
    "    cos(a,b) = \\frac{\\langle a,b\\rangle}{\\|a\\|\\|b\\|} = \\frac{aとbの内積}{aの大きさ、bの大きさ}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 上の数式で理解しなくともコサインで理解すれば簡単!\n",
    "\n",
    "|θ|0°|30°|45°|60°|90°|120°|135°|150°|180°|\n",
    "|:-|:-|:-|:-|:-|:-|:-|:-|:-|:-|\n",
    "|cosθ|1|$$ \\frac{\\sqrt3}{2} $$|$$ \\frac{1}{\\sqrt2} $$|$$ \\frac{1}{2} $$|0|$$ -\\frac{1}{2} $$|$$ -\\frac{1}{\\sqrt2} $$|$$ -\\frac{\\sqrt3}{2} $$|-1|\n",
    "\n",
    "- コサイン類似度は-1〜1をとりますが、Bag-of-Wordsの手法ではマイナス要素のベクトルが作られないので、0〜1の値の範囲が算出されます\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cs = cosine_similarity(bag.toarray(),bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_cs = pd.DataFrame(cs, index=['A','B','C'], columns=['A','B','C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# コサイン類似度の算出結果\n",
    "\n",
    "- 比較文書\n",
    "\n",
    "    - Aさん：ステーキが好きです。ハンバーグも好きです。\n",
    "    - Bさん：ステーキがとても好きです。\n",
    "    - Cさん：パスタが好きです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>A</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>C</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B        C\n",
       "A  1.000000  0.866025  0.57735\n",
       "B  0.866025  1.000000  0.50000\n",
       "C  0.577350  0.500000  1.00000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 今後の展望\n",
    "- コサイン類似度\n",
    "    - この後発表を控えている宮澤研究を楽しみにしてください。\n",
    "- トーン分析（感情分析）\n",
    "    - 極性辞書の単純集計型\n",
    "    - ナイーブベイズ分類器（機械学習の一つ）でネガポジを推計\n",
    "    - RNN（リカレントニューラルネットワーク）でネガポジを推計\n",
    "    - ファイナンスに特化した日本語極性辞書の作成    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
